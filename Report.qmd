---
title: "Heart attack Risk"
author: 
  - Nastaran  Mesgari
  - Sanaz Achik
  - Fateme Rajaeian
date: 2023-12-22
abstract: "Executive Summary.change abstract "
format: 
  html:
    code-fold: true
    standalone: true
    embed-resources: true
    toc: true
---
## 1- Introduction
 We're studying what factors affect the chance of having a heart attack. We want to understand how certain things, like age and lifestyle, relate to the likelihood of a heart attack. Our aim is to improve preventive measures and find indicators for heart health.

The Heart Attack Risk Prediction Dataset serves as a valuable resource for delving into the intricate dynamics of heart health and its predictors. Heart attacks, or myocardial infarctions, continue to be a significant global health issue, necessitating a deeper comprehension of their precursors and potential mitigating factors. This dataset encapsulates a diverse range of attributes including age, cholesterol levels, blood pressure, smoking habits, exercise patterns, dietary preferences, and more, aiming to elucidate the complex interplay of these variables in determining the likelihood of a heart attack. By employing predictive analytics and machine learning on this dataset, researchers and healthcare professionals can work towards proactive strategies for heart disease prevention and management. The dataset stands as a testament to collective efforts to enhance our understanding of cardiovascular health and pave the way for a healthier future.

The data comes from kaggel site, and there are a lot of columns that gathering in the following we explain it. 

Our goal is to uncover insights that can guide public health strategies and personalized healthcare. In the next sections, we'll explain how we process the data and build models to answer our research question and contribute to heart health understanding.

## 2. Data Set
### 2.1	Heart Attack Data set
The data set is about Heart Attack.there are 8763 rows and 26 Columns.(dtypes: float64(3), int64(16), object(7))
## 3. Data Pre-Processing

in this step we import the main libraries that we need 
```{python}
#| label: import-libraries
# importing main libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import warnings
from sklearn.feature_selection import VarianceThreshold
warnings.filterwarnings("ignore")
```

To use data, we need to import them and read the data. In this case, our data is CSV files, and it is in the folder whose name is data.
```{python}
#| label: data-import
# Import your data here.
df = pd.read_csv('data\PrimaryDataSet.csv')
df.head() 
```

```{python}
#| label: show how many rows and coloumn in data set 
df.shape
```

```{python}
#| label: Get inforaion
# getting data on dataset
df.info()
```

### 3.1	Data Cleaning
The steps followed for the data set is given below:
#### 3.1.1 Dropping unnecessary columns and rows:
Dropping unnecessary columns and rows is a data preprocessing step that involves removing specific columns or rows from a dataset that are deemed unnecessary for the analysis or modeling task at hand. This process is beneficial for several reasons. Reducing Dimensionality, Improving Computational Efficiency, Enhancing Model Performance, and so on .
 


#### 3.1.2 Checking for missing values
In most of the cases, we do not get complete datasets. They either have some values missing from the rows and columns or they do not have standardized values.
Dropping unnecessary columns and rows is a data preprocessing step that involves removing specific columns or rows from a dataset that are deemed unnecessary for the analysis or modeling task at hand. This process is beneficial for several reasons. Reducing Dimensionality, Improving Computational Efficiency, Enhancing Model Performance, and so on .
So, before going ahead with the analysis, it is a good idea to check whether the dataset has any missing values.As you can see in the output of the block below, there is no missing value in this dataset

```{python}
# Drop rows with null values
df = df.dropna()
#df.info()
#df.shape
```

```{python}
# Checking for missing values in the entire dataset
missing_values = df.isnull().sum()
# Printing the result
print(missing_values)
```


#### 	3.1.3 Checking for garbage values
Garbage value is generally a term meaning that the value in a variable doesn't have some sort of planned meaning.
By checking the statistical information of the data, some variables have negative values, and some have 0 values which are not compatible with the definition (corresponding to the dataset).
The detail of these values is given in the following tables:
 *** Negative Values and Ziro for deleting ***
 by this code we can check the data for minus and zero if it is not compatible by the meaning they have.
 ```{python}
 # get name the columns
 df.columns
 ```
```{python}
# Checking the negative values of age
#df[df['age']<0]['age'].value_counts()
df['age'].value_counts()
```
```{python}
# Checking the negative values of Sex

df['sex'].value_counts()
```

```{python}
# Checking the 0 values of fare_amount
df[df['age']==0]['age'].value_counts()
```


we checked the data , and there are any value of minus and zero so the rows not be changed . 


#### 	3.1.4 Checking the distribution of each variable 
Checking the distribution of each variable involves examining the spread and pattern of values within individual columns or features in a dataset. Understanding the distribution helps you gain insights into the central tendencies, variability, and shape of the data. This is crucial for making informed decisions during data analysis and modeling. Common statistical measures used to describe the distribution include mean, median, and standard deviation.

In this phase, first we checked the numeric variables with zero variance (threshold = 0), *they do not have any contribution on the model*.at this data we do not have any standard deviation equal to zero. so this step we didnt drop anything.

```{python}
# check the distribution for each column
df.describe().T
```

```{python}
df_numeric = df.select_dtypes(include=np.number)
df_numeric.shape
```

```{python}

# finding zero variance variables
selector_vr= VarianceThreshold(threshold=0)
selector_vr.fit_transform(df_numeric)
selector_vr.get_support(indices=True)
```

check the standard deviation if there are value near the zero , we can delete it . 

```{python}
#
df.head()
df.columns
```

now we check the data type object for showing the frequency of top elemans in each feature.



```{python}
# Specify the columns you want to include in the analysis
selected_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']

# Create a DataFrame containing only the selected columns
selected_df = df[selected_columns]

# Plot histograms for each variable
selected_df.hist(figsize=(10, 8))
plt.suptitle('Histograms of Selected Variables', y=0.95)
plt.show()


```
```{python}
# Displaying some statistics about categorical data
df.describe(include='object')
```

### 3.2 Data Transformation

#### 3.2.1 Transforming the categorical variables

If we have a column that is object for example yes or no question or if we have Boolean, we can convert them to integer.in this stage I created the new column. 
in this step we change type of ‘fbs’ from object to integer for using the model.
Encoding for fbs: 1 for 'yes', 0 for 'no'
I use the new data frame for own data with own selected column for better decision in others steps.
```{python} 
df.describe()
```


```{python}
"""Ordinal Encoding for fbs: 1:yes, 0:no,"""
#using Map Function
df2 = df.copy()

# Ordinal Encoding for fbs: 1 for 'yes', 0 for 'no'
ordinal_map = {'Yes': 1, 'No': 0}
df2['fbs'] = df2['fbs'].map(ordinal_map)
df2['fbs'] = df2['fbs'].astype(int)

```

```{python}
"""Ordinal Encoding for fbs: 1:yes, 0:no,"""
#using Map Function
df2 = df2.copy()

# Ordinal Encoding for target: 1 for 'yes', 0 for 'no'
ordinal_map = {'Yes': 1, 'No': 0}
df2['target'] = df2['target'].map(ordinal_map)
df2['target'] = df2['target'].astype(int)

``` 
```{python}
df2.info()
#df2.head()
df2.shape
```



```{python}
#Replace 'column_name' with the actual column name 

# Get unique values in the column
unique_values = df2['target'].unique()
print(unique_values)
unique_values = df2['slope'].unique()
# Display unique values to inspect
print(unique_values)

```


#### 3.2.2 Normalization, standardization, scaling

The data normalization process lowers the scale and brings all the data-points on the same scale.
Normalization: It involves scaling the values of a variable to a specific range, usually between 0 and 1
Standardization (Z-score normalization): It transforms the data to have a mean of 0 and a standard deviation of 1.
Scaling: is a general term for any transformation that alters the range of the data.
I use scaling in the model selection 

## 4 Data Exploration

We put our visualization here.

### 4.1 Finding outliers , dummy variable

the majority of the data. The use of outlier detection methods depends on the type of data and the analysis objective. However, generally, these methods are commonly applied to numerical columns or continuous variables. The reason for this is that the concept of outliers is more definable in continuous variables, and statistical measures such as mean, standard deviation, box plots, can easily be employed for their identification.
for founding the outliers of discrete and categorical variables we need to find the type of variables are integer or objects. We find the outliers and the Q1, Q3 and compare it with data and count how much of each independent variable out of this range and recognize and virtualize it .
For each integer variable, I use boxplot and histogram for visualization. The plots are as bellows:
```{python}
df2.columns
```

```{python}
intdf2 = df2.select_dtypes(include='integer')
intdf2.columns
```


we find the outliers and the Q1 , Q3 and compaire it with data and count how much of each independent variable out of this range and recognize and virtualized it . 


**age**

box plots, are a great chart to use when showing the distribution of data points,
As the plot shows, in this dataset, nearly 50% of the samples are in the age range of 48 to 61.
```{python}
plt.figure(figsize=(10,3))
plt.subplot(121)
plt.hist(np.array(df2['age']) , density=True , bins=50, edgecolor='black' ,facecolor='pink', alpha=0.75)
plt.xlabel('Value', fontsize= 10)
plt.ylabel('Frequency', fontsize= 10)
plt.subplot(122)
sns.boxplot(y ='age', data=df2,palette="Blues")
plt.xlabel('age')
plt.show()
```

```{python}
#calculate IQR and show thw outliers
Q1 = df2['age'].quantile(0.25)
Q3 = df2['age'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)
IQR = Q3 - Q1
print("IQR:",IQR)
Out1=Q1 - 1.5 * IQR
print("Out1:",Out1)
Out2=Q3 + 1.5 * IQR
print("Out2:",Out2)
outliers_absences_Port = (df2['age'] < Q1 - 1.5 * IQR) | (df2['age'] > Q3 + 1.5 * IQR)
# Filter the DataFrame to get the actual outlier values
outliers_absences_Port  = df2.loc[outliers_absences_Port, 'age']
# Display the outlier values
#print("df2 :", outliers_absences_Port)
```

```{python}
# check the columns we can see outliers
toll_df = df2.loc[df2['age']>80.5]
toll_df.shape

```

we find the outliers and the Q1 , Q3 and compaire it with data and count how much of each independent variable out of this range and recognize and virtualized it .
For each continuous variable, I use boxplot and histogram for visualization. The plots are as bellows:
 we check the IQR= Q3-Q1 and if the value is greater or less than of 1.5 IQR we can find the outlier and show the shape of them ( rows , columns) if existed .

 for getting data are continuous variable we need found the float variable 
 

 ```{python}
 float_df2 = df2.select_dtypes(include='float')
float_df2.columns
 ```


***'cp ***
```{python}
plt.figure(figsize=(10,3))
plt.subplot(121)
plt.hist(np.array(df2['cp']) , density=True , bins=50, edgecolor='black' ,facecolor='pink', alpha=0.75)
plt.xlabel('Value', fontsize= 10)
plt.ylabel('Frequency', fontsize= 10)
plt.subplot(122)
sns.boxplot(y ='cp', data=df,palette="Blues")
plt.xlabel('cp')
plt.show()
```


```{python}
#calculate IQR and show thw outliers
Q1 = df2['cp'].quantile(0.25)
Q3 = df2['cp'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)
IQR = Q3 - Q1
print("IQR:",IQR)
Out1=Q1 - 1.5 * IQR
print("Out1:",Out1)
Out2=Q3 + 1.5 * IQR
print("Out2:",Out2)
Out1=df2['cp'] < Q1 - 1.5 * IQR
outliers_cp  = (df2['cp'] < Q1 - 1.5 * IQR) | (df['cp'] > Q3 + 1.5 * IQR)
# Filter the DataFrame to get the actual outlier values
outliers_cp  = df.loc[outliers_cp, 'cp']
# Display the outlier values
print("cp :",outliers_cp)
```
```{python}
# check the columns we can see outliers
dis = df2.loc[df2['cp']>5]
dis.shape
```


***oldpeak ***
```{python}
plt.figure(figsize=(10,3))
plt.subplot(121)
plt.hist(np.array(df2['oldpeak']) , density=True , bins=50, edgecolor='black' ,facecolor='pink', alpha=0.75)
plt.xlabel('Value', fontsize= 10)
plt.ylabel('Frequency', fontsize= 10)
plt.subplot(122)
sns.boxplot(y ='oldpeak', data=df2,palette="Blues")
plt.xlabel('oldpeak')
plt.show()
```


```{python}
#calculate IQR and show thw outliers
Q1 = df2['oldpeak'].quantile(0.25)
Q3 = df2['oldpeak'].quantile(0.75)
print("Q1:",Q1)
print("Q3:",Q3)
IQR = Q3 - Q1
print("IQR:",IQR)
Out1=Q1 - 1.5 * IQR
print("Out1:",Out1)
Out2=Q3 + 1.5 * IQR
print("Out2:",Out2)
Out1=df2['oldpeak'] < Q1 - 1.5 * IQR
outliers_oldpeak  = (df2['oldpeak'] < Q1 - 1.5 * IQR) | (df2['oldpeak'] > Q3 + 1.5 * IQR)
# Filter the DataFrame to get the actual outlier values
outliers_oldpeak  = df2.loc[outliers_oldpeak, 'cp']
# Display the outlier values
print("oldpeak :",outliers_oldpeak)
```
```{python}
# check the columns we can see outliers
dis = df2.loc[df2['oldpeak']>4.5]
dis.shape
```



*** Bar charts for discrete and categorical variables ***

for founding the outliers of  discrete and categorical variables we need to find the type of variables are integer or objects . 
For each discrete variable (including Booleans), I used bar charts highlighting the target variable for each value. 

```{python}
int_df = df2.select_dtypes(include='integer')
int_df.columns
```

*** slope ***
```{python}
plt.figure(figsize=(5,5))
valuetable = pd.crosstab(df2['sex'],df2['target']) # ,normalize='index'
valuetable.plot.bar(stacked=True)
plt.title('sex / target')
plt.xlabel('sex')
plt.ylabel('target')
```

```{python}
df2['sex'].value_counts()
```

,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

Dummy variables

If we have categorical variables, we need to change our categorical variables into dummy variables (using get_dummies or OneHotEncoder).
for finding the corrolation the independent variable should be integer so this step we can check the variables are integer, 
```{python}
intdf = df2.select_dtypes(include='integer')
intdf.columns
```


```{python}

# Checking the linear correlation of variables
df3=df2[[ 
'age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalach', 'exang','ca', 'thal','target']]
corr_matrix = df3.corr()
corr_matrix['target'].sort_values(ascending = False)
```


### 4.2 Correlation between different features:
Correlation is the way of understanding the strength of the relationship between 2 variables or features in a dataset. Correlation coefficients determine this strength by indicating a value between [-1,1] where -1 indicates a very strong negative relationship, 0 indicates no relationship and 1 indicates strong positive relationship. Pearson correlation is one of the most widely used correlation method and it indicates the linear relationship between 2 variables. The heatmap of correlation between all variables of the dataset is given bellow:
```{python}
#| label: Get correlation as a table
df3[['age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalach', 'exang','ca', 'thal', 'target',]].corr()
```

```{python}
#| label: Get correlation visually
plt.figure(figsize = (19,10))
sns.heatmap(df3[['age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalach', 'exang',
       'ca', 'thal', 'target']].corr(), cmap="YlGnBu",
            annot=True)
```


**Key Findings**

Heart Attack risk has *highest correlation* with **Diabetes,BP_Systolic, Cholestrol and Exercise Hours Per Weak** because we have greatest number near to 1
Heart attack Risk is not much dependent on Sedentary Hours Per Day because it is the nearest number to -1 = -.0056
Alcohol Consumption has no stronger link with Heart Attack Risk because it is  -.014 
Smoking is not a major cause of Heart Attack= -.0041
The sorted correlation matrix for the target variable is as follows:
```{python}
print(df3.columns)
df2.info()
```


#### 4.2.1 Dummy variables
If we have categorical variables, and we want to fit a linear or logistic regression model, we need to change our categorical variables into dummy variables (using get_dummies or OneHotEncoder).
 
```{python}
"""One_hot encoding for 'slope' """
df2 = pd.get_dummies(df2, columns = ['slope'])
```

```{python}
#df.dtypes
df2.info()
#df2.columns
```


```{python}
"""Converting 'Object' and 'Boolean' Datatype into int"""
cat_columns = ['slope_High','slope_Low','slope_Moderate']
df2[cat_columns] = df2[cat_columns].astype(int)
```



```{python}
# Checking the linear correlation of variables
df2=df2[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'ca', 'thal', 'target', 'slope_High', 'slope_Low',
       'slope_Moderate']]
corr_matrix = df2.corr()
corr_matrix['target'].sort_values(ascending = False)
```

```{python}
#| label: Get correlation visually
plt.figure(figsize = (19,10))
sns.heatmap(df2[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'ca', 'thal', 'slope_High', 'slope_Low',
       'slope_Moderate', 'target']].corr(), cmap="YlGnBu",
            annot=True)
```

#### 4.3 Hypotheses
we check the data base on relation and the questions 


```{python}

# Summary statistics
summary_stats = df2
print(summary_stats)

# Visualize the distribution of the target variable (assuming binary: 0 for no heart attack, 1 for heart attack)
sns.countplot(x='target', data=df2)
plt.title('Distribution of cp  for Heart Attacks')
plt.show()

# Explore the relationship between age and the presence of heart attacks
sns.histplot(x='cp', hue='target', data=df2, kde=True)
plt.title('Distribution of cp for Heart Attacks')
plt.show()
```


***Checking the Hypotheses base on cp***
```{python}
# Checking the Hypotheses base on cp
# Formulate Hypotheses
# Hypothesis 1: There is a significant difference in the cp distribution between patients with and without heart attacks.

# Perform a t-test to test the hypothesis
cp_heart_attack = df2[df2['target'] == 1]['cp']
cp_no_heart_attack = df2[df2['target'] == 0]['cp']

t_stat, p_value = ttest_ind(cp_heart_attack, cp_no_heart_attack)
print(f"T-statistic: {t_stat}, p-value: {p_value}")

# Set significance level
alpha = 0.05

# Check if the p-value is less than the significance level
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in the cp distribution.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in the cp distribution.")


```


***Checking the Hypotheses base on oldpeak***
```{python}
# Checking the Hypotheses base on oldpeak
# Formulate Hypotheses
# Hypothesis 1: There is a significant difference in the oldpeak distribution between patients with and without heart attacks.

# Perform a t-test to test the hypothesis
oldpeak_heart_attack = df2[df2['target'] == 1]['oldpeak']
oldpeak_no_heart_attack = df2[df2['target'] == 0]['oldpeak']

t_stat, p_value = ttest_ind(oldpeak_heart_attack, oldpeak_no_heart_attack)
print(f"T-statistic: {t_stat}, p-value: {p_value}")

# Set significance level
alpha = 0.05

# Check if the p-value is less than the significance level
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in the oldpeak distribution.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in the oldpeak distribution.")


```



## 5. Data Analysis (Visualization and checking the distribution of each variable)
### 5.1 Data Modeling
```{python}
df['target'].unique()
df['target'].value_counts()
```

Now that the data is clean and the values of our target variable is balanced (0:5624, 1:3139), it is time to choose our classifier. A summary of each algorithm is described below.
**Logistic Regression** :  is a classification method used when the Response column is categorical with only two possible values. The probability of the possible outcomes is modeled with a logistic transformation as a weighted sum of the Predictor columns. The weights or regression coefficients are selected to maximize the likelihood of the observed data.
**Linear Discriminant Analysis** or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique that is commonly used for supervised classification problems. It is used for modelling differences in groups i.e. separating two or more classes. It is used to project the features in higher dimension space into a lower dimension space. Linear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data
K-Nearest Neighbors algorithm, also known as KNN or k-NN, is a non-parametric algorithm (which means it does not make any assumption on underlying data), supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. a class label is assigned based on a majority vote.
Decision Tree is a type of supervised machine learning used to categorize or make predictions based on how a previous set of questions were answered. The model is a form of supervised learning, meaning that the model is trained and tested on a set of data that contains the desired categorization. The tree can be explained by two entities, namely decision nodes and leaves.
Random Forest is a collection (a.k.a. ensemble) of many decision trees. A decision tree is a flow chart which separates data based on some condition. If a condition is true, you move on a path otherwise, you move on to another path.
At the first step of modeling, I decided to select 22 independent variables to put in the model. The variables are as follows:

```{python}
df2.columns
```

```{python}
X = df2[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'ca', 'thal', 'slope_High', 'slope_Low',
       'slope_Moderate']]
        
y=df2['target'].values
```

```{python}
X.head()
```

```{python}
print(y[1:10])
```

### 5.2	Model Selection
***Checking the impact of *Heart Attack Risk* variable with modeling *** 
In order to select my classifier, I performed a 10-fold cross validation algorithm on the above-mentioned classification models and calculated the accuracy (average of all 10 folds) of each model. The result of the cross validation is as follows:


```{python}
#| label: cross validation without removing Heart Attack Risk
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd


# Selecting features and target variable
x = df2.drop(['target' ], axis=1)
y = df2['target']

# Splitting the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

# Scaling the features using StandardScaler
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
xtest_scaled = scaler.transform(xtest)

# Defining classification models
models = [
    ('LR', LogisticRegression(n_jobs=-1)),
    ('LDA', LinearDiscriminantAnalysis()),
    ('KNN', KNeighborsClassifier()),
    ('DTC', DecisionTreeClassifier()),
    ('RFM', RandomForestClassifier(n_jobs=-1))
]

# 10-fold cross-validation
results = []
names = []

for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = cross_val_score(model, xtrain_scaled, ytrain, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)

    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

```

The results show that all the classification algorithms except Linear Discriminant Analysis and K-Nearest Neighbors have an accuracy of 64%. The result is maybe good, if we have almost all the classifiers giving a perfect result at the first step and without tuning the hyperparameters and feature engineering it is not normal.
To get the better result we should be surveyed precisely. Our target variable had the strongest linear correlation with variable “Diabetes”,” Cholesterol”,” BP_Systolic”,” Exercise Hours Per Week”. 



***RFC with cp and oldpeak...*** 

```{python}
# RandomForestClassifier without removing cp and oldpeak  prediction and classification report and score
x = df2.drop(['target' ], axis=1)
y = df2['target']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=20, n_jobs=-1)
model.fit(xtrain,ytrain)
ypred = model.predict(xtest)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(ytest, ypred)
print(cm)

from sklearn.metrics import classification_report
print(classification_report(ytest, ypred))

score = model.score(xtest, ytest)
print(score)
```

RFC with Diabetes,...: accuracy : 62%

```{python}
# ROC Curve
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(ytest, ypred)
auc
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(ytest, ypred)
fpr
tpr
thresholds
plt.plot([0, 1], [0, 1],'--')
plt.plot(fpr, tpr, marker='.')
plt.title('ROC-AUC')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

```

```{python}
auc = roc_auc_score(ytest, model.predict(xtest))
auc
```

To check this more precisely, I removed Diabetes, Chlostrol,BP_Systolic,'Exercise Hours Per Week'  from my feature variables and performed a Random Forest Classifier model. I split my data set into train (80% of the observation) and test (20% of the observation), fitted the model on train data and performed a prediction on my test data. The classification report is as follows; the accuracy reduced into 62%.

*** RFC without  cp , oldpeak*** 
```{python}
# RandomForestClassifier with removing cp , oldpeak
x = df2.drop(['cp','oldpeak'],axis=1)
y = df2['target']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=20, n_jobs=-1)
model.fit(xtrain,ytrain)
ypred = model.predict(xtest)

cm = confusion_matrix(ytest, ypred)
print(cm)

print(classification_report(ytest, ypred))

score = model.score(xtest, ytest)
print(score)
```


```{python}
# ROC Curve
auc = roc_auc_score(ytest, ypred)
auc
fpr, tpr, thresholds = roc_curve(ytest, ypred)
fpr
tpr
thresholds
plt.plot([0, 1], [0, 1],'--')
plt.plot(fpr, tpr, marker='.')
plt.title('ROC-AUC')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
```

```{python}
auc = roc_auc_score(ytest, model.predict(xtest))
auc
```

```{python}
df2.info()
```

***Choosing the model using 10-fold Cross validation, based on the accuracy metric*** 

 10 fold cross validation of classification models with removing cp

```{python}
    # Selecting features and target variable
x = df2.drop(['cp','oldpeak' ], axis=1)
y = df2['target']

# Splitting the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

# Scaling the features using StandardScaler
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
xtest_scaled = scaler.transform(xtest)

# Defining classification models
models = [
    ('LR', LogisticRegression(n_jobs=-1)),
    ('LDA', LinearDiscriminantAnalysis()),
    ('KNN', KNeighborsClassifier()),
    ('DTC', DecisionTreeClassifier()),
    ('RFM', RandomForestClassifier(n_jobs=-1))
]

# 10-fold cross-validation
results = []
names = []

for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = cross_val_score(model, xtrain_scaled, ytrain, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)

    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

```

In summary, based on mean accuracy and standard deviation:
Logistic Regression, Linear Discriminant Analysis, and Random Forest Classifier seem to perform relatively well with higher mean accuracies and lower variability.
K-Nearest Neighbors has a lower mean accuracy and slightly higher variability.
Decision Tree Classifier has a lower mean accuracy and relatively low variability.I chose Logistic Regression as my classifier.
### 6	Results and Conclusions
#### 6.1	Fitting the model

**Dropping columns and fitting the model**
```{python}
# LogisticRegression model without 'Cholesterol','Heart Attack Risk','Diabetes','BP_Systolic',
x = df2.drop(['Cholesterol','Heart Attack Risk','Diabetes','BP_Systolic','Heart Attack Risk', ], axis=1)
y = df2['target']
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)
#model = LogisticRegression(solver= 'lbfgs', penalty='l2', C= 0.3, n_jobs=-1) # C: inverse of regularization, smaller value, stronger regularization
model = KNeighborsClassifier(solver= 'lbfgs', penalty='l2', C= 0.3, n_jobs=-1)
model.fit(xtrain , ytrain)
```




By looking at the coefficients of the model, I keep below model and check the model performance with new features.(greater)
•	   Alcohol Consumption
•	Sedentary Hours Per Day
•	Obesity
•	Diabetes  


```{python}
print(df2.columns)
```

**Testing the model by reducing the number of features regarding their coefficient in the first model.** 
```{python}
# improved LogisticRegression model, classification report and score
x = df2.drop([ 
        'age', 'sex','slope_High', 'slope_Low',
       'slope_Moderate'
       ], axis=1)
y = df2['target']

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

model = LogisticRegression(solver= 'lbfgs', penalty='l2', C= 0.3, n_jobs=-1) # C: inverse of regularization, smaller value, stronger regularization

model.fit(xtrain , ytrain)
ypred = model.predict(xtest)

cm = confusion_matrix(ytest, ypred)
print(cm)

print(classification_report(ytest, ypred))

score = model.score(xtest, ytest)
print(score)
```



```{python}
# improved LogisticRegression model ROC-AUC
auc = roc_auc_score(ytest, ypred)
auc

fpr, tpr, thresholds = roc_curve(ytest, ypred)
fpr
tpr
thresholds
plt.plot([0, 1], [0, 1], linestyle='--')
plt.plot(fpr, tpr, marker='.')
plt.title('ROC-AUC')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
```

```{python}
auc = roc_auc_score(ytest, model.predict(xtest))
auc
```




```{python}

#trained Logistic Regression model
coefficients = pd.concat([pd.DataFrame(x.columns), pd.DataFrame(np.transpose(model.coef_))], axis=1)
column_name = ['Dependent Variable', 'Coefficient']
coefficients.columns = column_name

# Add a new row for the intercept
coefficients.loc[len(coefficients)] = ['Intercept', model.intercept_[0]]

# Display the coefficients DataFrame
print(coefficients)

```

```{python}
# 10 fold cross validation of classification models after improvment

x = df2.drop([ 'cp','oldpeak', ], axis=1)
y = df2['target']

# Splitting the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

# Scaling the features using StandardScaler
scaler = StandardScaler()
xtrain_scaled = scaler.fit_transform(xtrain)
xtest_scaled = scaler.transform(xtest)

# Defining classification models
models = [
    ('LR', LogisticRegression(n_jobs=-1)),
    ('LDA', LinearDiscriminantAnalysis()),
    ('KNN', KNeighborsClassifier()),
    ('DTC', DecisionTreeClassifier()),
    ('RFM', RandomForestClassifier(n_jobs=-1))
]

# 10-fold cross-validation
results = []
names = []

for name, model in models:
    kfold = KFold(n_splits=10, shuffle=True, random_state=42)
    cv_results = cross_val_score(model, xtrain_scaled, ytrain, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)

    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)

```